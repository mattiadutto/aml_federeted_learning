{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Baseline implementation","metadata":{"id":"KRyFWIxiPEVX"}},{"cell_type":"code","source":"# download the Cifar10 non-iid splits, if not present\n\nfrom os import path\nimport urllib.request\nimport zipfile\n\nif not path.exists(\"cifar10\"):\n    save_path = \"cifar10.zip\"\n    urllib.request.urlretrieve(\"http://storage.googleapis.com/gresearch/federated-vision-datasets/cifar10.zip\", save_path)\n    \n    with zipfile.ZipFile(save_path, 'r') as zip_ref:\n        zip_ref.extractall(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:20:00.204801Z","iopub.execute_input":"2022-01-17T09:20:00.205470Z","iopub.status.idle":"2022-01-17T09:20:00.775070Z","shell.execute_reply.started":"2022-01-17T09:20:00.205331Z","shell.execute_reply":"2022-01-17T09:20:00.773923Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"config = {\n    \"E\": 5, # number of local epochs\n    \"K\": 10, # number of clients selected each round\n    \"NUMBER_OF_CLIENTS\": 100, # total number of clients\n    \"MAX_TIME\": 500,\n    \"BATCH_SIZE\": 50,\n    \"LR\": 0.1,\n    \"DATA_DISTRIBUTION\": \"iid\", # \"iid\" | \"non-iid\"\n    \"DIRICHELET_ALPHA\": 0.00, # 0.00, 0.05, 0.10, 0.20, 0.50, 1.00, 10.00, 100.0\n    \"ALPHA\": 0.01,\n    \"LR_DECAY\": 1, # [0.992, 0.998, 1.0]\n    \"LOG_FREQUENCY\": 5,\n    \"WEIGHT_DECAY\": 1e-3\n}","metadata":{"execution":{"iopub.status.busy":"2022-01-17T10:22:51.170630Z","iopub.execute_input":"2022-01-17T10:22:51.170927Z","iopub.status.idle":"2022-01-17T10:22:51.177402Z","shell.execute_reply.started":"2022-01-17T10:22:51.170893Z","shell.execute_reply":"2022-01-17T10:22:51.176433Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2022-01-17T10:22:51.677485Z","iopub.execute_input":"2022-01-17T10:22:51.678269Z","iopub.status.idle":"2022-01-17T10:22:51.683892Z","shell.execute_reply.started":"2022-01-17T10:22:51.678187Z","shell.execute_reply":"2022-01-17T10:22:51.682733Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"# From: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\nclass Net(nn.Module):\n\n    def __init__(self, *, input_size=32):\n        super(Net, self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square convolution\n        self.conv1 = nn.Conv2d(3, 64, 5)\n        self.conv2 = nn.Conv2d(64, 64, 5)\n        \n        # output of the conv layer is (w', h') = (w - 5 + 1, h - 5 + 1)\n        # max_pool2d halves the dimensions (w', h') = (w / 2, h / 2)\n\n        # dynamically compute the image size\n        size = input_size // 4 - 3\n        self.fc1 = nn.Linear(64 * (size * size), 384)\n        self.fc2 = nn.Linear(384, 192)\n        self.fc3 = nn.Linear(192, 10)\n\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square, you can specify with a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nnet = Net()\nnet = net.to(device)\nprint(net)","metadata":{"id":"5Ar1b6p68b3G","outputId":"e695f455-87f7-44c6-fe6f-fb6bdf6b300d","execution":{"iopub.status.busy":"2022-01-17T10:22:51.876738Z","iopub.execute_input":"2022-01-17T10:22:51.877097Z","iopub.status.idle":"2022-01-17T10:22:51.901215Z","shell.execute_reply.started":"2022-01-17T10:22:51.877068Z","shell.execute_reply":"2022-01-17T10:22:51.900213Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\nfrom copy import deepcopy\n\nclass Client():\n  def __init__(self, i, train_set, validation_set, *, input_size=32):\n    self.i = i\n    self.train_loader = torch.utils.data.DataLoader(train_set, batch_size=config[\"BATCH_SIZE\"],\n                                         shuffle=True, num_workers=0)\n    self.validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=config[\"BATCH_SIZE\"],\n                                         shuffle=False, num_workers=0)\n    self.net = Net(input_size=input_size)\n    self.net = self.net.to(device)\n    # create your optimizer\n    self.optimizer = optim.SGD(self.net.parameters(), lr=config[\"LR\"])\n    self.criterion = nn.CrossEntropyLoss()\n    # wandb.watch(self.net, criterion=self.criterion, log_freq=100, log_graph=False)\n\n    self.previous_parameters = {key:torch.zeros(params.shape, device=device) for key, params in self.net.state_dict().items()}\n    self.previous_gradient = {key:torch.zeros(params.shape, device=device) for key, params in self.net.state_dict().items()}\n    \n  def clientUpdate(self, lr, alpha, parameters):\n    self.net.load_state_dict(parameters)\n    for g in self.optimizer.param_groups:\n      g['lr'] = lr\n\n    for e in range(config[\"E\"]):\n      for images, labels in self.train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # in your training loop:\n        self.optimizer.zero_grad()   # zero the gradient buffers\n        output = self.net(images)\n        \n        # compute the loss of the model\n        loss = self.criterion(output, labels)\n        \n        # compute the dot product term\n        loss -= sum(\n            torch.sum(old_grad * cur_params) \n            for (old_grad, cur_params) \n            in zip(self.previous_gradient.values(), self.net.state_dict().values())\n        )\n        \n        loss += (alpha / 2) * sum(\n            # torch.sum(cur_params * cur_params) \n            torch.linalg.norm(cur_params.reshape(-1) - old_params.reshape(-1), 2) ** 2\n            for cur_params, old_params\n            in zip(self.net.state_dict().values(), self.previous_parameters.values())\n        )\n\n        loss.backward()\n        \n        # wandb.log({f\"client-loss-{self.i}\": loss.item()})\n        self.optimizer.step()    # Does the update\n    \n    # store the previous gradient\n    self.previous_gradient = {\n      key: old_grad - alpha * (cur_params - old_params)\n      for (key, old_grad), cur_params, old_params in zip(self.previous_gradient.items(), self.net.state_dict().values(), self.previous_parameters.values())\n    }\n\n    self.previous_parameters = deepcopy(self.net.state_dict())\n\n    return self.net.state_dict()\n\n  def compute_accuracy(self, parameters):\n    self.net.load_state_dict(parameters)\n\n    running_corrects = 0\n    n = 0\n    for data, labels in self.validation_loader:\n        data = data.to(device)\n        labels = labels.to(device)\n\n        outputs = self.net(data)\n\n        _, preds = torch.max(outputs.data, 1)\n\n        running_corrects += torch.sum(preds == labels.data).data.item()\n        n += len(preds)\n\n                \n    return running_corrects / n\n","metadata":{"id":"eCwGFjE5Feil","execution":{"iopub.status.busy":"2022-01-17T10:22:52.412884Z","iopub.execute_input":"2022-01-17T10:22:52.413337Z","iopub.status.idle":"2022-01-17T10:22:52.548132Z","shell.execute_reply.started":"2022-01-17T10:22:52.413300Z","shell.execute_reply":"2022-01-17T10:22:52.546216Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"[v.shape for v in clients[0].previous_parameters.values()]","metadata":{"execution":{"iopub.status.busy":"2022-01-17T10:22:52.865747Z","iopub.execute_input":"2022-01-17T10:22:52.866040Z","iopub.status.idle":"2022-01-17T10:22:52.874933Z","shell.execute_reply.started":"2022-01-17T10:22:52.866009Z","shell.execute_reply":"2022-01-17T10:22:52.873850Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"[v.shape for v in clients[0].net.state_dict().values()]","metadata":{"execution":{"iopub.status.busy":"2022-01-17T10:22:53.902195Z","iopub.execute_input":"2022-01-17T10:22:53.903009Z","iopub.status.idle":"2022-01-17T10:22:53.911480Z","shell.execute_reply.started":"2022-01-17T10:22:53.902975Z","shell.execute_reply":"2022-01-17T10:22:53.910494Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n\ndef parse_csv(filename):\n  splits = defaultdict(lambda: [])\n  with open(filename) as f:\n    for line in f:\n      if not line[0].isdigit():\n        continue\n\n      user_id, image_id, _ = (int(token) for token in line.split(\",\"))\n      splits[user_id].append(image_id)\n\n  return splits\n","metadata":{"id":"QcsNecZCVPP-","execution":{"iopub.status.busy":"2022-01-17T10:22:54.287907Z","iopub.execute_input":"2022-01-17T10:22:54.288766Z","iopub.status.idle":"2022-01-17T10:22:54.296220Z","shell.execute_reply.started":"2022-01-17T10:22:54.288730Z","shell.execute_reply":"2022-01-17T10:22:54.295228Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"import torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nimport random\nfrom statistics import mean\n\nfrom tqdm.notebook import tqdm\n\nrandom.seed(42)\n\ntrain_transform = transforms.Compose([\n  transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntest_transform = transforms.Compose([\n  transforms.ToTensor(),\n  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=train_transform)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=test_transform)\n\n\nif config[\"DATA_DISTRIBUTION\"] == \"iid\":\n  # split the training set\n  trainset_len = ( len(trainset) // config[\"NUMBER_OF_CLIENTS\"] ) * config[\"NUMBER_OF_CLIENTS\"]\n  trainset = torch.utils.data.Subset(trainset, list(range(trainset_len)))\n\n  lengths = len(trainset) // config[\"NUMBER_OF_CLIENTS\"] * np.ones(config[\"NUMBER_OF_CLIENTS\"], dtype=int)\n  trainsets = torch.utils.data.random_split(dataset=trainset, lengths=lengths)\nelse:\n  dirichelet_splits = parse_csv(f\"cifar10/federated_train_alpha_{config['DIRICHELET_ALPHA']:.2f}.csv\")\n  trainsets = [torch.utils.data.Subset(trainset, indices) for indices in dirichelet_splits.values()]\n\n\n# split the validation set\ntestset_len = ( len(testset) // config[\"NUMBER_OF_CLIENTS\"] ) * config[\"NUMBER_OF_CLIENTS\"]\ntestset = torch.utils.data.Subset(testset, list(range(testset_len)))\n\nlengths = len(testset) // config[\"NUMBER_OF_CLIENTS\"] * np.ones(config[\"NUMBER_OF_CLIENTS\"], dtype=int)\ntestsets = torch.utils.data.random_split(dataset=testset, lengths=lengths)\n\n\nclientsSizes = torch.zeros(config[\"NUMBER_OF_CLIENTS\"])\nclients = list()\n\ndef selectClients(k):\n  return random.choices(clients, k=k)\n\ndef aggregateClient(deltaThetas):\n  parameters = None\n  for i,d in enumerate(deltaThetas):\n    #ratio = len(trainsets[i])/len(trainset)\n    ratio = len(trainsets[i])/(len(trainsets[i])*config['K'])\n    \n    if i == 0:\n      parameters = {k:ratio*v for k, v in d.items()}\n    else:\n      for (k, v) in d.items():\n        parameters[k] += ratio * v\n   \n  return parameters\n\nfor c in range(config[\"NUMBER_OF_CLIENTS\"]):\n  clients.append(Client(c, trainsets[c], testsets[c]))\n\n# initial learning rate\nlr = config[\"LR\"]\n\n# collect the test accuracies over the epochs\ntest_accuracies = []\n\n# initialize h_0\nh = {key:torch.zeros(params.shape, device=device) for key, params in net.state_dict().items()}\n\nm = config[\"NUMBER_OF_CLIENTS\"]\nalpha = config[\"ALPHA\"]\nK = config[\"K\"]\n\nfor step in tqdm(range(config[\"MAX_TIME\"])):\n#for t in range(MAX_TIME):\n  selected_clients = selectClients(K)\n  #print(f\"Client(s) {[client.i for client in selected_clients]} selected\")\n\n  thetas = list()\n  for i, c in enumerate(selected_clients):\n    thetas.append(c.clientUpdate(lr, alpha, net.state_dict()))\n  \n  h = {\n    key: prev_h - alpha * 1 / m * sum(theta[key] - old_params for theta in thetas)\n    for (key, prev_h), old_params in zip(h.items(), net.state_dict().values())\n  }\n\n  new_parameters = {\n    key: (1 / K) * sum(theta[key] for theta in thetas)\n    for key in net.state_dict().keys()\n  }\n\n  new_parameters = {\n    key: params - (1 / alpha) * h_params\n    for (key, params), h_params in zip(new_parameters.items(), h.values())\n  }\n\n  net.load_state_dict(new_parameters)\n\n  lr *= config[\"LR_DECAY\"]\n\n  if step % config[\"LOG_FREQUENCY\"] == 0:\n    model_parameters = net.state_dict()\n    avg_accuracy = mean(client.compute_accuracy(model_parameters) for client in clients)\n    test_accuracies.append(avg_accuracy)\n\n    print(f\"Average accuracy after {step} rounds is {avg_accuracy}\")","metadata":{"id":"gTmw78eJ-EW5","outputId":"9bad269e-aa08-45ed-f934-27c805e12b15","execution":{"iopub.status.busy":"2022-01-17T10:22:54.858223Z","iopub.execute_input":"2022-01-17T10:22:54.858484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean","metadata":{"execution":{"iopub.status.busy":"2022-01-17T09:37:00.328838Z","iopub.status.idle":"2022-01-17T09:37:00.330052Z","shell.execute_reply.started":"2022-01-17T09:37:00.329722Z","shell.execute_reply":"2022-01-17T09:37:00.329753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statistics import mean\n\nmodel_parameters = net.state_dict()\navg_accuracy = mean(client.compute_accuracy(model_parameters) for client in clients)\ntest_accuracies.append(avg_accuracy)\n\nprint(f\"Average accuracy after {config['MAX_TIME']} rounds is {avg_accuracy}\")","metadata":{"id":"utb7E9snmQwi","outputId":"a121bd78-f871-49bf-e0f9-6029319ac03f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport json\n\ntimestr = time.strftime(\"%Y_%m_%d-%I_%M_%S_%p\")\nartifact_filename = f\"artifacts/server_model-{timestr}\"\n\n# parameters of the trained model\nserver_model = net.state_dict()\n# save the model on the local file system\ntorch.save(server_model, artifact_filename + \".pth\")\n\ndata = {\n    \"config\": config,\n    \"test_accuracies\": test_accuracies\n}\n\nwith open(artifact_filename + \".json\", \"w\") as f:\n    f.write(json.dumps(data, indent=4))\n\n# save the model on wandb\n# wandb.save(artifact_filename)\n# Finish the wandb session and upload all data\n# wandb.finish(0, quiet=False)","metadata":{},"execution_count":null,"outputs":[]}]}