{"cells":[{"cell_type":"markdown","metadata":{"id":"KRyFWIxiPEVX"},"source":["# Baseline implementation"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-02-08T08:22:07.364439Z","iopub.status.busy":"2022-02-08T08:22:07.364175Z","iopub.status.idle":"2022-02-08T08:22:07.370403Z","shell.execute_reply":"2022-02-08T08:22:07.369226Z","shell.execute_reply.started":"2022-02-08T08:22:07.364410Z"},"id":"NMMwxdgGTSnZ","trusted":true},"outputs":[],"source":["# download the Cifar10 non-iid splits, if not present\n","\n","import os\n","import urllib.request\n","import zipfile\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","if not os.path.exists(\"cifar10\"):\n","    save_path = \"cifar10.zip\"\n","    urllib.request.urlretrieve(\"http://storage.googleapis.com/gresearch/federated-vision-datasets/cifar10_v1.1.zip\", save_path)\n","    \n","    with zipfile.ZipFile(save_path, 'r') as zip_ref:\n","        zip_ref.extractall(\"cifar10\")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-02-08T08:22:07.388406Z","iopub.status.busy":"2022-02-08T08:22:07.387960Z","iopub.status.idle":"2022-02-08T08:22:07.394842Z","shell.execute_reply":"2022-02-08T08:22:07.393770Z","shell.execute_reply.started":"2022-02-08T08:22:07.388375Z"},"id":"9CnFYAwMTSne","trusted":true},"outputs":[],"source":["config = {\n","    \"E\": 5,  # number of local epochs\n","    \"K\": 5,  # number of clients selected each round # [5, 10, 20]\n","    \"NUMBER_OF_CLIENTS\": 100,  # total number of clients\n","    \"MAX_TIME\": 1000,\n","    \"BATCH_SIZE\": 50,\n","    \"VALIDATION_BATCH_SIZE\": 500,\n","    \"LR\": 0.01,\n","    \"WEIGHT_DECAY\": 0,\n","    \"DATA_DISTRIBUTION\": \"iid\",  # \"iid\" | \"non-iid\"\n","    \"DIRICHELET_ALPHA\":  [\"iid\"], # [0.50, 1.00, 10.00, 100.0],\n","    \"AVERAGE_ACCURACY\": np.zeros(8),\n","    \"FED_AVG_M\": False,\n","    \"FED_AVG_M_BETA\": 0.9,\n","    \"FED_AVG_M_GAMMA\": 1,\n","    \"LR_DECAY\": 0.99,\n","    \"LOG_FREQUENCY\": 25,\n","    \"AUGMENTATION_PROB\": 0.0,\n","    \"ALPHA\": 1e-2,  # for FedDyn\n","    \"SAVE_FREQUENCY\": 500,\n","    \"NORM_LAYER\": \"\",  # batch normalization\n","}\n","\n","use_data_preloading = True\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-02-08T08:22:07.401422Z","iopub.status.busy":"2022-02-08T08:22:07.400750Z","iopub.status.idle":"2022-02-08T08:22:07.412638Z","shell.execute_reply":"2022-02-08T08:22:07.411837Z","shell.execute_reply.started":"2022-02-08T08:22:07.401392Z"},"id":"5Ar1b6p68b3G","trusted":true},"outputs":[],"source":["# From: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(3, 64, 5)\n","        if config[\"NORM_LAYER\"] == \"bn\":\n","            self.norm1 = nn.BatchNorm2d(64)\n","        elif config[\"NORM_LAYER\"] == \"gn\":\n","            self.norm1 = nn.GroupNorm(4, 64)\n","\n","        self.conv2 = nn.Conv2d(64, 64, 5)\n","        if config[\"NORM_LAYER\"] == \"bn\":\n","            self.norm2 = nn.BatchNorm2d(64)\n","        elif config[\"NORM_LAYER\"] == \"gn\":\n","            self.norm2 = nn.GroupNorm(4, 64)\n","\n","        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n","        self.fc2 = nn.Linear(384, 192)\n","        self.fc3 = nn.Linear(192, 10)\n","\n","    def forward(self, x):\n","        if config[\"NORM_LAYER\"] in [\"bn\", \"gn\"]:\n","            x = F.max_pool2d(F.relu(self.norm1(self.conv1(x))), (2, 2))\n","            x = F.max_pool2d(F.relu(self.norm2(self.conv2(x))), 2)\n","        else:\n","            x = F.max_pool2d(F.relu(self.conv1(x)), kernel_size=(2, 2), stride=2)\n","            x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size=2, stride=2)\n","\n","        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","\n","# print(net)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-02-08T08:22:07.636583Z","iopub.status.busy":"2022-02-08T08:22:07.636337Z","iopub.status.idle":"2022-02-08T08:22:07.664762Z","shell.execute_reply":"2022-02-08T08:22:07.664112Z","shell.execute_reply.started":"2022-02-08T08:22:07.636555Z"},"id":"eCwGFjE5Feil","trusted":true},"outputs":[],"source":["import torch.optim as optim\n","\n","\n","class Client:\n","    def __init__(\n","        self,\n","        i,\n","        train_set,\n","        validation_set,\n","        *,\n","        input_size=32,\n","        use_data_preloading,\n","        train_transform\n","    ):\n","        self.i = i\n","        self.net = Net()\n","        self.net = self.net.to(device)\n","\n","        # Create the validation loader\n","        self.validation_loader = torch.utils.data.DataLoader(\n","            validation_set, batch_size=len(validation_set), shuffle=False, num_workers=0\n","        )\n","\n","        # Create an optimizer for the model's parameters\n","        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n","\n","        self.train_transform = train_transform\n","\n","        self.use_data_preloading = use_data_preloading\n","        if self.use_data_preloading:\n","            # preloading train and validation data\n","            self.train_loader = torch.utils.data.DataLoader(\n","                train_set,\n","                batch_size=len(train_set),\n","                shuffle=True,\n","                num_workers=0,\n","                pin_memory=True,\n","            )\n","\n","            # preload the training images\n","            training_images, training_labels = next(iter(self.train_loader))\n","            self.training_images = training_images.to(device)\n","            self.training_labels = training_labels.to(device)\n","\n","            # preload the validation images\n","            validation_images, validation_labels = next(iter(self.validation_loader))\n","            self.validation_images = validation_images.to(device)\n","            self.validation_labels = validation_labels.to(device)\n","        else:\n","            self.train_loader = torch.utils.data.DataLoader(\n","                train_set,\n","                batch_size=config[\"BATCH_SIZE\"],\n","                shuffle=True,\n","                num_workers=0,\n","                pin_memory=True,\n","            )\n","\n","        self.previous_gradient = {\n","            key: torch.zeros(params.shape, device=device)\n","            for key, params in self.net.state_dict().items()\n","        }\n","\n","    def clientUpdate(self, alpha, parameters):\n","        self.net.load_state_dict(parameters)\n","        self.net.train(True)\n","        \n","        self.optimizer = optim.SGD(\n","            self.net.parameters(), lr=config[\"LR\"], weight_decay=config[\"WEIGHT_DECAY\"]\n","        )\n","\n","        for _ in range(config[\"E\"]):\n","            epoch_loss, n = 0, 0\n","\n","            for images, labels in self.iter_training_data():\n","                # in your training loop:\n","                self.optimizer.zero_grad()  # zero the gradient buffers\n","                output = self.net(images)\n","\n","                # compute the loss of the model\n","                loss = self.criterion(output, labels)\n","                epoch_loss += loss.item()\n","                \n","                n += labels.size(0)\n","                loss = loss / labels.size(0)\n","                \n","                prev_grad_flat = torch.cat([p.reshape(-1) for p in self.previous_gradient.values()])\n","                cur_flat = torch.cat([p.reshape(-1) for p in self.net.state_dict().values()])\n","                par_flat = torch.cat([p.reshape(-1) for p in parameters.values()])\n","\n","                # compute the penalty terms\n","                linear_penalty = torch.sum(prev_grad_flat * cur_flat)\n","                # norm_penalty = (alpha / 2.0) * (torch.linalg.norm(cur_flat - par_flat, 1) ** 2)\n","                # norm_penalty = (alpha / 2.0) * torch.sum((cur_flat - par_flat) ** 2)\n","                norm_penalty = (alpha / 2) * torch.linalg.norm(cur_flat - par_flat, 2) ** 2\n","\n","                # print(nloss, nn_loss)\n","                # print(loss, llin_loss, nn_loss)\n","                loss = loss - linear_penalty + norm_penalty\n","                # loss_algo = alpha * torch.sum(cur_flat * (-par_flat + prev_grad_flat))\n","                #loss = loss + loss_algo\n","                loss.backward()\n","                \n","                torch.nn.utils.clip_grad_norm_(parameters=self.net.parameters(), max_norm=10)\n","\n","                self.optimizer.step()  # Does the update\n","            epoch_loss = epoch_loss / n\n","\n","        # store the previous gradient\n","        self.previous_gradient = {\n","            key: old_grad - alpha * (cur_params - old_params)\n","            for (key, old_grad), cur_params, old_params in zip(\n","                self.previous_gradient.items(),\n","                self.net.state_dict().values(),\n","                parameters.values(),\n","            )\n","        }\n","\n","        return self.net.state_dict()\n","\n","    def iter_training_data(self):\n","        batch_size = config[\"BATCH_SIZE\"]\n","\n","        if self.use_data_preloading:\n","            # shuffle the training data\n","            indices = torch.randperm(self.training_images.size(0))\n","            training_images, training_labels = (\n","                self.training_images[indices],\n","                self.training_labels[indices],\n","            )\n","\n","            # possibly apply the training transformation\n","            if self.train_transform is not None:\n","                training_images = torch.stack(\n","                    [self.train_transform(im) for im in training_images]\n","                )\n","\n","            yield from zip(\n","                torch.split(training_images, batch_size),\n","                torch.split(training_labels, batch_size),\n","            )\n","        else:\n","            yield from self.train_loader\n","\n","    def iter_validation_data(self):\n","        batch_size = config[\"VALIDATION_BATCH_SIZE\"]\n","\n","        if self.use_data_preloading:\n","            yield from zip(\n","                torch.split(self.validation_images, batch_size),\n","                torch.split(self.validation_labels, batch_size),\n","            )\n","        else:\n","            yield from self.validation_loader\n","\n","    def compute_accuracy(self, parameters):\n","        self.net.load_state_dict(parameters)\n","        self.net.eval()\n","\n","        running_corrects = 0\n","        loss, n = 0, 0\n","        for data, labels in self.iter_validation_data():\n","            with torch.no_grad():\n","                outputs = self.net(data)\n","            loss += self.criterion(outputs, labels).item()\n","\n","            _, preds = torch.max(outputs.data, 1)\n","\n","            running_corrects += torch.sum(preds == labels.data).data.item()\n","            n += len(preds)\n","\n","        return loss / n, running_corrects / n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-02-08T08:22:07.667000Z","iopub.status.busy":"2022-02-08T08:22:07.666721Z","iopub.status.idle":"2022-02-08T08:22:07.677056Z","shell.execute_reply":"2022-02-08T08:22:07.676230Z","shell.execute_reply.started":"2022-02-08T08:22:07.666962Z"},"id":"QcsNecZCVPP-","trusted":true},"outputs":[],"source":["from collections import defaultdict\n","\n","\n","def parse_csv(filename):\n","    splits = defaultdict(lambda: [])\n","    with open(filename) as f:\n","        for line in f:\n","            if not line[0].isdigit():\n","                continue\n","\n","            user_id, image_id, _ = (int(token) for token in line.split(\",\"))\n","            splits[user_id].append(image_id)\n","\n","    return splits"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-02-08T08:22:07.679964Z","iopub.status.busy":"2022-02-08T08:22:07.679786Z","iopub.status.idle":"2022-02-08T08:22:07.688189Z","shell.execute_reply":"2022-02-08T08:22:07.687322Z","shell.execute_reply.started":"2022-02-08T08:22:07.679942Z"},"id":"6FwJ-K1ATSno","trusted":true},"outputs":[],"source":["import time\n","import json\n","import numpy\n","from copy import deepcopy\n","\n","\n","def listToString(l):\n","    return \" \".join(str(l))\n","\n","\n","def printJSON(alpha, acc, net, step=None):\n","    artifacts_dir = \"artifacts\"\n","\n","    artifact_filename = f\"FEDDYN_ALPHA_{alpha}_E_{config['E']}_K_{config['K']}\"\n","    if step is not None:\n","        artifact_filename += f\"_STEPS_{step}\"\n","\n","    if config[\"AUGMENTATION_PROB\"] > 0:\n","        artifact_filename += f\"_T\"\n","\n","    artifact_filename += (\n","        f\"_{config['NORM_LAYER'].upper()}\" if config[\"NORM_LAYER\"] else \"\"\n","    )\n","\n","    # parameters of the trained model\n","    server_model = net.state_dict()\n","    # save the model on the local file system\n","    torch.save(server_model, f\"{artifacts_dir}/{artifact_filename}.pth\")\n","    config_copy = deepcopy(config)\n","    config_copy[\"DIRICHELET_ALPHA\"] = listToString(config_copy[\"DIRICHELET_ALPHA\"])\n","    config_copy[\"AVERAGE_ACCURACY\"] = numpy.array2string(\n","        config_copy[\"AVERAGE_ACCURACY\"]\n","    )\n","    data = {\"config\": config_copy, \"alpha\": listToString(alpha), \"accuracy\": acc}\n","\n","    with open(f\"{artifacts_dir}/{artifact_filename}.json\", \"w\") as f:\n","        f.write(json.dumps(data, indent=4))\n","\n","    # If you want to cat the file, my suggestion is to avoid this is a pretty heavy operation at least on my pc\n","    # artifact_filename += \".json\"\n","    #!cat artifact_filename"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-02-08T08:22:07.689854Z","iopub.status.busy":"2022-02-08T08:22:07.689413Z","iopub.status.idle":"2022-02-08T08:22:09.287365Z","shell.execute_reply":"2022-02-08T08:22:09.286557Z","shell.execute_reply.started":"2022-02-08T08:22:07.689819Z"},"id":"hCgfQtAlTSnq","outputId":"93f6824c-534d-4d8d-af2e-62b95a329074","trusted":true},"outputs":[],"source":["import torchvision\n","import torchvision.transforms as transforms\n","import numpy as np\n","import random\n","from statistics import mean\n","\n","from tqdm.notebook import tqdm\n","\n","random.seed(42)\n","\n","# Normalization values for the CIFAR10 dataset\n","normalization_transform = transforms.Normalize(\n","    mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262]\n",")\n","\n","# Transformation strategy:\n","#  1. apply as many transformations as possibile offline (at trainset level)\n","#  2. apply the remaining transformations online (at client level, before\n","#     iterating over the data)\n","#\n","# Random transformations must be applied online.\n","\n","if use_data_preloading:\n","    # data preloading is enabled\n","\n","    if config[\"AUGMENTATION_PROB\"] > 1e-5:\n","        # non-zero augmentation probability => apply transformations online\n","        offline_train_transform = []\n","        online_train_transform = transforms.Compose(\n","            [\n","                transforms.RandomApply([transforms.RandomHorizontalFlip(1)], config[\"AUGMENTATION_PROB\"]),\n","                transforms.RandomApply([transforms.RandomCrop(32, padding=4)], config[\"AUGMENTATION_PROB\"]),\n","                normalization_transform,\n","            ]\n","        )\n","    else:\n","        # zero augmentation probability => apply transformations offline\n","        offline_train_transform = [normalization_transform]\n","        online_train_transform = None\n","else:\n","    # augmentation probability is zero => all transformations can be applied offline\n","    offline_train_transform = [transforms.Compose(\n","        [\n","            transforms.RandomApply([transforms.RandomHorizontalFlip(1)], config[\"AUGMENTATION_PROB\"]),\n","            transforms.RandomApply([transforms.RandomCrop(32, padding=4)], config[\"AUGMENTATION_PROB\"]),\n","            normalization_transform,\n","        ]\n","    )]\n","    online_train_transform = None\n","\n","\n","trainset = torchvision.datasets.CIFAR10(\n","    root=\"./data\",\n","    train=True,\n","    download=True,\n","    #transform=transforms.Compose([transforms.ToTensor()]),\n","    transform=transforms.Compose([transforms.ToTensor(), *offline_train_transform]),\n",")\n","\n","testset = torchvision.datasets.CIFAR10(\n","    root=\"./data\",\n","    train=False,\n","    download=True,\n","    #transform=transforms.Compose([transforms.ToTensor()]),\n","    transform=transforms.Compose([transforms.ToTensor(), normalization_transform]),\n",")\n","\n","if not os.path.exists(\"artifacts\"):\n","    os.mkdir(\"artifacts\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-08T08:22:09.290290Z","iopub.status.busy":"2022-02-08T08:22:09.289838Z"},"id":"gTmw78eJ-EW5","outputId":"402a3847-ea25-494e-d8fb-ba504b0db624","trusted":true},"outputs":[],"source":["for alpha_i, alpha in enumerate(config[\"DIRICHELET_ALPHA\"]):\n","    net = Net()\n","    net = net.to(device)\n","\n","    if config[\"DATA_DISTRIBUTION\"] == \"iid\":\n","        # split the training set\n","        trainset_len = (len(trainset) // config[\"NUMBER_OF_CLIENTS\"]) * config[\n","            \"NUMBER_OF_CLIENTS\"\n","        ]\n","        trainset = torch.utils.data.Subset(trainset, list(range(trainset_len)))\n","\n","        lengths = (\n","            len(trainset)\n","            // config[\"NUMBER_OF_CLIENTS\"]\n","            * np.ones(config[\"NUMBER_OF_CLIENTS\"], dtype=int)\n","        )\n","        trainsets = torch.utils.data.random_split(dataset=trainset, lengths=lengths)\n","    else:\n","        dirichelet_splits = parse_csv(f\"cifar10/federated_train_alpha_{alpha:.2f}.csv\")\n","        trainsets = [\n","            torch.utils.data.Subset(trainset, indices)\n","            for indices in dirichelet_splits.values()\n","        ]\n","\n","    # split the validation set\n","    testset_len = (len(testset) // config[\"NUMBER_OF_CLIENTS\"]) * config[\n","        \"NUMBER_OF_CLIENTS\"\n","    ]\n","    testset = torch.utils.data.Subset(testset, list(range(testset_len)))\n","\n","    lengths = (\n","        len(testset)\n","        // config[\"NUMBER_OF_CLIENTS\"]\n","        * np.ones(config[\"NUMBER_OF_CLIENTS\"], dtype=int)\n","    )\n","    testsets = torch.utils.data.random_split(dataset=testset, lengths=lengths)\n","\n","    clientsSizes = torch.zeros(config[\"NUMBER_OF_CLIENTS\"])\n","    clients = list()\n","\n","    def selectClients(k):\n","        return random.sample(clients, k=k)\n","\n","    def aggregateClient(deltaThetas):\n","        parameters = None\n","        for i, d in enumerate(deltaThetas):\n","            # ratio = len(trainsets[i])/len(trainset)\n","            ratio = len(trainsets[i]) / (len(trainsets[i]) * config[\"K\"])\n","\n","            if i == 0:\n","                parameters = {k: ratio * v for k, v in d.items()}\n","            else:\n","                for (k, v) in d.items():\n","                    parameters[k] += ratio * v\n","\n","        return parameters\n","\n","    for c in range(config[\"NUMBER_OF_CLIENTS\"]):\n","        clients.append(\n","            Client(\n","                c,\n","                trainsets[c],\n","                testsets[c],\n","                use_data_preloading=use_data_preloading,\n","                train_transform=online_train_transform,\n","            )\n","        )\n","\n","    # initial learning rate\n","    lr = config[\"LR\"]\n","\n","    # collect the test accuracies over the epochs\n","    test_accuracies = []\n","\n","    # initialize h_0\n","    h = {\n","        key: torch.zeros(params.shape, device=device)\n","        for key, params in net.state_dict().items()\n","    }\n","\n","    m = config[\"NUMBER_OF_CLIENTS\"]\n","    feddyn_alpha = config[\"ALPHA\"]\n","    K = config[\"K\"]\n","\n","    accuracies = list()\n","\n","    # bese model\n","    best_model = {}\n","    best_accuracy = 0.0   \n","\n","    for step in tqdm(range(config[\"MAX_TIME\"])):\n","        # for t in range(MAX_TIME):\n","        selected_clients = selectClients(K)\n","        # print(f\"Client(s) {[client.i for client in selected_clients]} selected\")\n","\n","        thetas = list()\n","        for i, c in enumerate(selected_clients):\n","            thetas.append(c.clientUpdate(feddyn_alpha, net.state_dict()))\n","\n","        h = {\n","            key: prev_h\n","            - feddyn_alpha * 1 / m * sum(theta[key] - old_params for theta in thetas)\n","            for (key, prev_h), old_params in zip(h.items(), net.state_dict().values())\n","        }\n","\n","        new_parameters = {\n","            key: (1 / K) * sum(theta[key] for theta in thetas)\n","            for key in net.state_dict().keys()\n","        }\n","\n","        new_parameters = {\n","            key: params - (1 / feddyn_alpha) * h_params\n","            for (key, params), h_params in zip(new_parameters.items(), h.values())\n","        }\n","\n","        net.load_state_dict(new_parameters)\n","\n","        if step % config[\"LOG_FREQUENCY\"] == 0:\n","            client_losses_accuracies = [\n","                client.compute_accuracy(new_parameters) for client in clients\n","            ]\n","            client_losses, client_accuracies = zip(*client_losses_accuracies)\n","\n","            avg_client_accuracy = mean(client_acc for client_acc in client_accuracies)\n","            accuracies.append(avg_client_accuracy * 100)\n","            print(f\"Average accuracy after {step} rounds is {avg_client_accuracy*100}\")\n","\n","            if step % config[\"SAVE_FREQUENCY\"] == 0:\n","                printJSON(alpha, accuracies, net, step)\n","\n","            if avg_client_accuracy >= best_accuracy:\n","                best_accuracy = avg_client_accuracy\n","                best_model = deepcopy(new_parameters)   \n","\n","        # Not used\n","        # avg_accuracy = mean(\n","        #     float(client.compute_accuracy(new_parameters)[1]) for client in clients\n","        # )\n","    \n","    avg_accuracy = mean(float(client.compute_accuracy(best_model)[1]) for client in clients)\n","  \n","\n","    config[\"AVERAGE_ACCURACY\"][alpha_i] = avg_accuracy\n","    print(\n","        f\"Average accuracy with alpha = {alpha} after {step+1} rounds is {avg_accuracy*100}\"\n","    )\n","    printJSON(alpha, accuracies, net)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KWMUrxVETSnt","trusted":true},"outputs":[],"source":["import shutil\n","\n","shutil.make_archive(\"artifacts\", \"zip\", \"artifacts\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
