{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRyFWIxiPEVX"
      },
      "source": [
        "# Federated Learning based on Dynamic Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-08T21:36:23.716952Z",
          "iopub.status.busy": "2022-02-08T21:36:23.716638Z",
          "iopub.status.idle": "2022-02-08T21:36:23.724007Z",
          "shell.execute_reply": "2022-02-08T21:36:23.722764Z",
          "shell.execute_reply.started": "2022-02-08T21:36:23.716921Z"
        },
        "id": "NMMwxdgGTSnZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from typing import *\n",
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "import json\n",
        "from statistics import mean\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import random\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VCG9bJOlnuw"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-08T21:36:23.732260Z",
          "iopub.status.busy": "2022-02-08T21:36:23.731951Z",
          "iopub.status.idle": "2022-02-08T21:36:23.740309Z",
          "shell.execute_reply": "2022-02-08T21:36:23.739563Z",
          "shell.execute_reply.started": "2022-02-08T21:36:23.732230Z"
        },
        "id": "9CnFYAwMTSne",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"E\": 5,  # number of local epochs\n",
        "    \"K\": 5,  # number of clients selected each round # [5, 10, 20]\n",
        "    \"NUMBER_OF_CLIENTS\": 100,  # total number of clients\n",
        "    \"MAX_TIME\": 1500, # number of rounds\n",
        "    \"BATCH_SIZE\": 50,\n",
        "    \"VALIDATION_BATCH_SIZE\": 500,\n",
        "    \"LR\": 0.01, # learning rate\n",
        "    \"WEIGHT_DECAY\": 4e-4,\n",
        "    \"DATA_DISTRIBUTION\": \"non-iid\",  # “iid” | “non-iid”\n",
        "    \"DIRICHELET_ALPHA\":  [0.0, 0.1, 0.5, 10.0], # [0.50, 1.00, 10.00, 100.0],\n",
        "    \"AVERAGE_ACCURACY\": np.zeros(4),\n",
        "    \"LOG_FREQUENCY\": 25, # frequency of logs printed\n",
        "    \"AUGMENTATION_PROB\": 0.0, # for data transformation, see below (Prepare dataset cells)\n",
        "    \"ALPHA\": 1e-3,  # for FedDyn\n",
        "    \"SAVE_FREQUENCY\": 500, # frequency of logs saved\n",
        "    \"NORM_LAYER\": \"gn\", # Normalization layer [None: \"\", Batch: \"bn\", Group: \"gn\"]\n",
        "}\n",
        "\n",
        "use_data_preloading = True\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKCCf-zlrkKS",
        "outputId": "ac4d939f-a8df-41c0-c275-fa25595c629d"
      },
      "outputs": [],
      "source": [
        "config_name = \"feddyn_gn\"\n",
        "artifacts_target_directory = f\"./artifacts/{config_name}\"\n",
        "\n",
        "# Uncomment the following lines to save the results on Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# artifacts_target_directory = f'/content/drive/MyDrive/artifacts/{config_name}'\n",
        "\n",
        "\n",
        "if not os.path.exists(f\"{artifacts_target_directory}/final\"):\n",
        "    os.makedirs(f\"{artifacts_target_directory}/final\")\n",
        "\n",
        "if not os.path.exists(f\"{artifacts_target_directory}/partials\"):\n",
        "    os.makedirs(f\"{artifacts_target_directory}/partials\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJWqMCr3lnuy"
      },
      "source": [
        "## CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsRzyrGslnuz"
      },
      "outputs": [],
      "source": [
        "# download the Cifar10 non-iid splits, if not present\n",
        "if not os.path.exists(\"cifar10\"):\n",
        "    save_path = \"cifar10.zip\"\n",
        "    urllib.request.urlretrieve(\"http://storage.googleapis.com/gresearch/federated-vision-datasets/cifar10_v1.1.zip\", save_path)\n",
        "    \n",
        "    with zipfile.ZipFile(save_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"cifar10\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CeVx9arlnuz",
        "outputId": "efbb896a-b970-4d5e-b978-480c0cfb98cf"
      },
      "outputs": [],
      "source": [
        "# Normalization values for the CIFAR10 dataset\n",
        "normalization_transform = transforms.Normalize(\n",
        "    mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262]\n",
        ")\n",
        "\n",
        "# Transformation strategy:\n",
        "#  1. apply as many transformations as possibile offline (at trainset level)\n",
        "#  2. apply the remaining transformations online (at client level, before\n",
        "#     iterating over the data)\n",
        "#\n",
        "# Random transformations must be applied online.\n",
        "\n",
        "if use_data_preloading:\n",
        "    # Data preloading is enabled\n",
        "\n",
        "    if config[\"AUGMENTATION_PROB\"] > 1e-5:\n",
        "        # Non-zero augmentation probability => apply transformations online\n",
        "        offline_train_transform = []\n",
        "        online_train_transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.RandomApply([transforms.RandomHorizontalFlip(1)], config[\"AUGMENTATION_PROB\"]),\n",
        "                transforms.RandomApply([transforms.RandomCrop(32, padding=4)], config[\"AUGMENTATION_PROB\"]),\n",
        "                normalization_transform,\n",
        "            ]\n",
        "        )\n",
        "    else:\n",
        "        # Zero augmentation probability => apply transformations offline\n",
        "        offline_train_transform = [normalization_transform]\n",
        "        online_train_transform = None\n",
        "else:\n",
        "    # Augmentation probability is zero => all transformations can be applied offline\n",
        "    offline_train_transform = [transforms.Compose(\n",
        "        [\n",
        "            transforms.RandomApply([transforms.RandomHorizontalFlip(1)], config[\"AUGMENTATION_PROB\"]),\n",
        "            transforms.RandomApply([transforms.RandomCrop(32, padding=4)], config[\"AUGMENTATION_PROB\"]),\n",
        "            normalization_transform,\n",
        "        ]\n",
        "    )]\n",
        "    online_train_transform = None\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([transforms.ToTensor(), *offline_train_transform]),\n",
        ")\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([transforms.ToTensor(), normalization_transform]),\n",
        ")\n",
        "\n",
        "assert(len(trainset) % config[\"NUMBER_OF_CLIENTS\"] == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-7bYPNolnu1"
      },
      "outputs": [],
      "source": [
        "def parse_splits_csv(filename: str) -> Tuple[DefaultDict[int, List[int]], Dict[int, int]]:\n",
        "    \"\"\"Read a CIFAR-10 splits file\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filename : str\n",
        "        path of the .csv file containing the splits\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[DefaultDict[int, List[int]], Dict[int, int]]\n",
        "        the dictionary containing the splits as user_id:[image_id]\n",
        "        and the labels_mapping as image_id:label\n",
        "    \"\"\"\n",
        "    splits = defaultdict(lambda: [])\n",
        "    labels_mapping = dict()\n",
        "\n",
        "    with open(filename) as f:\n",
        "        for line in f:\n",
        "            if not line[0].isdigit():\n",
        "                # Skip the first line\n",
        "                continue\n",
        "\n",
        "            user_id, image_id, label = (int(token) for token in line.split(\",\"))\n",
        "            splits[user_id].append(image_id)\n",
        "            labels_mapping[image_id] = label\n",
        "\n",
        "    return splits, labels_mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdcURDHwlnu2"
      },
      "source": [
        "## LeNet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-08T21:36:23.750695Z",
          "iopub.status.busy": "2022-02-08T21:36:23.750496Z",
          "iopub.status.idle": "2022-02-08T21:36:23.763255Z",
          "shell.execute_reply": "2022-02-08T21:36:23.762254Z",
          "shell.execute_reply.started": "2022-02-08T21:36:23.750672Z"
        },
        "id": "5Ar1b6p68b3G",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# From: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
        "        if config[\"NORM_LAYER\"] == \"bn\": # if batch normalization\n",
        "            self.norm1 = nn.BatchNorm2d(64)\n",
        "        elif config[\"NORM_LAYER\"] == \"gn\": # if group normalization\n",
        "            self.norm1 = nn.GroupNorm(4, 64)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 64, 5)\n",
        "        if config[\"NORM_LAYER\"] == \"bn\":\n",
        "            self.norm2 = nn.BatchNorm2d(64)\n",
        "        elif config[\"NORM_LAYER\"] == \"gn\":\n",
        "            self.norm2 = nn.GroupNorm(4, 64)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.fc3 = nn.Linear(192, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if config[\"NORM_LAYER\"] in [\"bn\", \"gn\"]:\n",
        "            x = F.max_pool2d(F.relu(self.norm1(self.conv1(x))), (2, 2))\n",
        "            x = F.max_pool2d(F.relu(self.norm2(self.conv2(x))), 2)\n",
        "        else:\n",
        "            x = F.max_pool2d(F.relu(self.conv1(x)), kernel_size=(2, 2), stride=2)\n",
        "            x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size=2, stride=2)\n",
        "            \n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEp0a139lnu3"
      },
      "source": [
        "## Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-08T21:36:24.002324Z",
          "iopub.status.busy": "2022-02-08T21:36:24.002021Z",
          "iopub.status.idle": "2022-02-08T21:36:24.030619Z",
          "shell.execute_reply": "2022-02-08T21:36:24.029652Z",
          "shell.execute_reply.started": "2022-02-08T21:36:24.002273Z"
        },
        "id": "eCwGFjE5Feil",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "class Client:\n",
        "    def __init__(self, i: int, train_set, validation_set, use_data_preloading=False, train_transform=None,\n",
        "        batch_size=config[\"BATCH_SIZE\"], validation_batch_size=config[\"VALIDATION_BATCH_SIZE\"],\n",
        "        epochs=config[\"E\"], lr=config[\"LR\"], weight_decay=config[\"WEIGHT_DECAY\"]):\n",
        "        \"\"\"Instantiate a new client\n",
        "\n",
        "        The parameter `use_data_preloading` allows to indicate whether the client should\n",
        "        preload all its training and validation data before starting the training loop. This\n",
        "        is preferred since it drastically speeds up the training proccess. Beware that, depending\n",
        "        on the dataset, the memory usage may quickly becoming unfeasible. 100 clients, each\n",
        "        with its own data and network, requires ~2.5 GB of gpu memory.\n",
        "        `train_transform` is a pytorch transform to be applied on each sample at training time.\n",
        "        \"\"\"\n",
        "        self.i = i\n",
        "        self.net = Net().to(device)\n",
        "\n",
        "        # Create the dataloader for the validation data\n",
        "        self.validation_loader = torch.utils.data.DataLoader(validation_set, \n",
        "            batch_size=len(validation_set), shuffle=False, num_workers=0\n",
        "        )\n",
        "\n",
        "        # Create the loss criterion\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "\n",
        "        self.train_transform = train_transform\n",
        "        self.batch_size = batch_size\n",
        "        self.validation_batch_size = validation_batch_size\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        self.use_data_preloading = use_data_preloading\n",
        "        if self.use_data_preloading:\n",
        "            # Preloading training data (the batch size config is ignored here)\n",
        "            self.train_loader = torch.utils.data.DataLoader(train_set,\n",
        "                batch_size=len(train_set), shuffle=True, \n",
        "                num_workers=0, pin_memory=True\n",
        "            )\n",
        "\n",
        "            # Read all the dataset at once...\n",
        "            training_images, training_labels = next(iter(self.train_loader))\n",
        "            # ...and transfer the data on the target device\n",
        "            self.training_images = training_images.to(device)\n",
        "            self.training_labels = training_labels.to(device)\n",
        "\n",
        "            # Preload the validation images\n",
        "            validation_images, validation_labels = next(iter(self.validation_loader))\n",
        "            # ...and transfer the data on the target device\n",
        "            self.validation_images = validation_images.to(device)\n",
        "            self.validation_labels = validation_labels.to(device)\n",
        "        else:\n",
        "            # Preloading was not requested => a dataloader is initialized as usual\n",
        "            self.train_loader = torch.utils.data.DataLoader(train_set,\n",
        "                batch_size=self.batch_size, shuffle=True, \n",
        "                num_workers=0, pin_memory=True\n",
        "            )\n",
        "\n",
        "        # Initialize the first gradient to zero\n",
        "        size = sum(p.numel() for p in self.net.parameters() if p.requires_grad)\n",
        "        self.previous_gradient = torch.zeros((size,), device=device)\n",
        "\n",
        "    def update(self, alpha: float, parameters: OrderedDict[str, torch.Tensor])->OrderedDict[str, torch.Tensor]:\n",
        "        \"\"\"Run a step of client training\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        alpha : float\n",
        "            FedDyn's alpha coefficient\n",
        "        parameters : OrderedDict[str, torch.Tensor]\n",
        "            Initial parameters of the client model\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        OrderedDict[str, torch.Tensor]\n",
        "            New parameters of the client at the end of the training step\n",
        "        \"\"\"\n",
        "        # Restore the parameters of the network from the server model\n",
        "        self.net.load_state_dict(parameters)\n",
        "        self.net.train(True)\n",
        "        \n",
        "        # Initialize the optimizer\n",
        "        self.optimizer = optim.SGD(self.net.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            # Compute the total epoch loss\n",
        "            epoch_loss, n = 0, 0\n",
        "            \n",
        "            for images, labels in self.iter_training_data():\n",
        "                # Set gradients to zero\n",
        "                self.optimizer.zero_grad()\n",
        "                output = self.net(images)\n",
        "\n",
        "                # Compute the loss of the model\n",
        "                loss = self.criterion(output, labels)\n",
        "                epoch_loss += loss.item()\n",
        "                \n",
        "                n += labels.size(0)\n",
        "                loss = loss / labels.size(0)\n",
        "                \n",
        "                # Flatten the current parameters\n",
        "                # NOTE: self.net.parameters() is used in place of state_dict since the tensors\n",
        "                #       returned by state_dict are NOT differentiable (requires_grad == False)\n",
        "                cur_flat = torch.cat([p.reshape(-1) for p in self.net.parameters()])\n",
        "                # Flatten the current server parameters\n",
        "                par_flat = torch.cat([p.reshape(-1) for k, p in parameters.items() if k in [k1 for k1, v in self.net.named_parameters()] ])\n",
        "                #assert(cur_flat.requires_grad)\n",
        "                \n",
        "                # Compute the linear penalty: prev_grad_flat · cur_flat\n",
        "                linear_penalty = torch.sum(self.previous_gradient * cur_flat)\n",
        "                # Compute the quadratic penalty: (alpha / 2) * || cur_flat - par_flat || ^ 2\n",
        "                norm_penalty = (alpha / 2) * torch.linalg.norm(cur_flat - par_flat, 2) ** 2\n",
        "                \n",
        "                # Compute the total mini-batch loss\n",
        "                loss = loss - linear_penalty + norm_penalty\n",
        "                \n",
        "                # Backward step\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(parameters=self.net.parameters(), max_norm=10)\n",
        "                self.optimizer.step()\n",
        "                                \n",
        "            epoch_loss = epoch_loss / n\n",
        "\n",
        "        # Preserve the current gradient for the next iteration\n",
        "        cur_flat = torch.cat([p.detach().reshape(-1) for p in self.net.parameters()])\n",
        "        self.previous_gradient -= alpha * (cur_flat - par_flat)\n",
        "        \n",
        "        # print(loss, linear_penalty, norm_penalty)\n",
        "        return self.net.state_dict()\n",
        "\n",
        "    def iter_training_data(self) -> Generator[torch.Tensor, Any, Any]:\n",
        "        \"\"\"Iterate over the training data of the client\n",
        "\n",
        "        Yields\n",
        "        -------\n",
        "        Generator[torch.Tensor]\n",
        "            the training dataset, split in mini-batches\n",
        "        \"\"\"\n",
        "        if self.use_data_preloading:\n",
        "            # Data already preloaded => shuffle and return from cache\n",
        "            indices = torch.randperm(self.training_images.size(0))\n",
        "            images, labels = self.training_images[indices], self.training_labels[indices]\n",
        "\n",
        "            # Possibly apply the required training transformation\n",
        "            if self.train_transform is not None:\n",
        "                images = torch.stack([self.train_transform(im) for im in images])\n",
        "\n",
        "            # Yield the training set\n",
        "            yield from zip(torch.split(images, self.batch_size), torch.split(labels, self.batch_size))\n",
        "        else:\n",
        "            # Data not already preloaded => yield from the dataloader\n",
        "            for images, labels in self.train_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                yield images, labels\n",
        "\n",
        "    def iter_validation_data(self) -> Generator[torch.Tensor, Any, Any]:\n",
        "        \"\"\"Iterate over the validation data of the the client\n",
        "\n",
        "        Yields\n",
        "        -------\n",
        "        Generator[torch.Tensor]\n",
        "            the validation dataset, split in mini-batches\n",
        "        \"\"\"\n",
        "        if self.use_data_preloading:\n",
        "            # Data already preloaded => return from cache\n",
        "            yield from zip(\n",
        "                torch.split(self.validation_images, self.validation_batch_size),\n",
        "                torch.split(self.validation_labels, self.validation_batch_size),\n",
        "            )\n",
        "        else:\n",
        "            # Data not already preloaded => yield from the dataloader\n",
        "            for images, labels in self.validation_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                yield images, labels\n",
        "\n",
        "    def compute_accuracy(self, parameters: OrderedDict[str, torch.Tensor]) -> Tuple[float, float]:\n",
        "        \"\"\"Compute the accuracy of the client\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        parameters : OrderedDict[str, torch.Tensor]\n",
        "            parameters of the client\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tuple[float, float]\n",
        "            average loss on the validation set, accuracy on the validation set\n",
        "        \"\"\"\n",
        "        self.net.load_state_dict(parameters)\n",
        "        # Set the model in evaluation mode\n",
        "        self.net.eval()\n",
        "\n",
        "        running_corrects = 0\n",
        "        loss, n = 0, 0\n",
        "        for data, labels in self.iter_validation_data():\n",
        "            with torch.no_grad():\n",
        "                # Compute the network outputs without gradients\n",
        "                outputs = self.net(data)\n",
        "            \n",
        "            # Compute the validation \n",
        "            loss += self.criterion(outputs, labels).item()\n",
        "\n",
        "            # Count the number of correct predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "            n += len(preds)\n",
        "\n",
        "        return loss / n, running_corrects / n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-08T21:36:24.045323Z",
          "iopub.status.busy": "2022-02-08T21:36:24.044863Z",
          "iopub.status.idle": "2022-02-08T21:36:24.054495Z",
          "shell.execute_reply": "2022-02-08T21:36:24.053796Z",
          "shell.execute_reply.started": "2022-02-08T21:36:24.045285Z"
        },
        "id": "6FwJ-K1ATSno",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def listToString(l):\n",
        "    return \" \".join(str(l))\n",
        "\n",
        "def printJSON(alpha, acc, net, step=None):\n",
        "    \"\"\"Create the json artifacts file\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha: float\n",
        "        value of alpha\n",
        "    acc: list\n",
        "        list of accuracies at different iterations\n",
        "    net: the actual network configuration\n",
        "    step: int\n",
        "        current value of iteration\n",
        "    \"\"\"\n",
        "    dirname = artifacts_target_directory\n",
        "    \n",
        "    artifact_filename = f\"FEDDYN_ALPHA_{alpha}_E_{config['E']}_K_{config['K']}\"\n",
        "    if step is not None:\n",
        "        dirname = f\"{dirname}/partials\"\n",
        "        artifact_filename += f\"_STEPS_{step}\"\n",
        "    else:\n",
        "        dirname = f\"{dirname}/final\"\n",
        "\n",
        "    if config[\"AUGMENTATION_PROB\"] > 0:\n",
        "        artifact_filename += f\"_T\"\n",
        "\n",
        "    artifact_filename += (\n",
        "        f\"_{config['NORM_LAYER'].upper()}\" if config[\"NORM_LAYER\"] else \"\"\n",
        "    )\n",
        "\n",
        "    # Parameters of the trained model\n",
        "    server_model = net.state_dict()\n",
        "    # Save the model on the local file system\n",
        "    torch.save(server_model, f\"{dirname}/{artifact_filename}.pth\")\n",
        "    config_copy = deepcopy(config)\n",
        "    config_copy[\"DIRICHELET_ALPHA\"] = listToString(config_copy[\"DIRICHELET_ALPHA\"])\n",
        "    config_copy[\"AVERAGE_ACCURACY\"] = np.array2string(\n",
        "        config_copy[\"AVERAGE_ACCURACY\"]\n",
        "    )\n",
        "    data = {\"config\": config_copy, \"alpha\": listToString(alpha), \"accuracy\": acc}\n",
        "\n",
        "    with open(f\"{dirname}/{artifact_filename}.json\", \"w\") as f:\n",
        "        f.write(json.dumps(data, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCSxAD5elnu7"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-08T21:36:25.715244Z",
          "iopub.status.busy": "2022-02-08T21:36:25.714954Z",
          "iopub.status.idle": "2022-02-08T21:37:08.440579Z",
          "shell.execute_reply": "2022-02-08T21:37:08.439333Z",
          "shell.execute_reply.started": "2022-02-08T21:36:25.715208Z"
        },
        "id": "gTmw78eJ-EW5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# In the FedDyn paper the number of clients is identified with m\n",
        "m = config[\"NUMBER_OF_CLIENTS\"]\n",
        "feddyn_alpha = config[\"ALPHA\"]\n",
        "K = config[\"K\"]\n",
        "\n",
        "# Collect the test accuracies over the different values of Dirichlet alpha\n",
        "accuracies = defaultdict(lambda: [])\n",
        "\n",
        "for alpha_i, alpha in enumerate(config[\"DIRICHELET_ALPHA\"]):\n",
        "    # Create a dummy model that will hold the server model parameters\n",
        "    net = Net().to(device)\n",
        "\n",
        "    # Prepare the training set\n",
        "    if config[\"DATA_DISTRIBUTION\"] == \"iid\":\n",
        "        lengths = [len(trainset) // m] * m\n",
        "        trainsets = torch.utils.data.random_split(dataset=trainset, lengths=lengths)\n",
        "    else:\n",
        "        dirichelet_splits, _ = parse_splits_csv(f\"cifar10/federated_train_alpha_{alpha:.2f}.csv\")\n",
        "        trainsets = [torch.utils.data.Subset(trainset, indices) for indices in dirichelet_splits.values()]\n",
        "\n",
        "    # Prepare the validation set\n",
        "    lengths = [len(testset) // m] * m\n",
        "    testsets = torch.utils.data.random_split(dataset=testset, lengths=lengths)\n",
        "\n",
        "    # Instantiate the clients\n",
        "    clients = [\n",
        "        Client(id, trainset, testset, use_data_preloading, online_train_transform)\n",
        "        for id, (trainset, testset) in enumerate(zip(trainsets, testsets))\n",
        "    ]\n",
        "\n",
        "    # Initialize h\n",
        "    h = {\n",
        "        key: torch.zeros(params.shape, device=device)\n",
        "        for key, params in net.state_dict().items()\n",
        "    }\n",
        "\n",
        "    # best model\n",
        "    best_model = {}\n",
        "    best_accuracy = 0.0   \n",
        "\n",
        "    for step in tqdm(range(config[\"MAX_TIME\"])):\n",
        "        selected_clients = random.sample(clients, K)\n",
        "        #print(f\"Client(s) {[client.i for client in selected_clients]} selected\")\n",
        "\n",
        "        # Collect the updates from the clients\n",
        "        thetas = [client.update(feddyn_alpha, net.state_dict()) for client in selected_clients]\n",
        "\n",
        "        h = {\n",
        "            key: prev_h\n",
        "            - feddyn_alpha * 1 / m * sum(theta[key] - old_params for theta in thetas)\n",
        "            for (key, prev_h), old_params in zip(h.items(), net.state_dict().values())\n",
        "        }\n",
        "\n",
        "        new_parameters = {\n",
        "            key: (1 / K) * sum(theta[key] for theta in thetas)\n",
        "            for key in net.state_dict().keys()\n",
        "        }\n",
        "\n",
        "        new_parameters = {\n",
        "            key: params - (1 / feddyn_alpha) * h_params\n",
        "            for (key, params), h_params in zip(new_parameters.items(), h.values())\n",
        "        }\n",
        "\n",
        "        net.load_state_dict(new_parameters)\n",
        "\n",
        "        # Compute the average accuracy\n",
        "        if step % config[\"LOG_FREQUENCY\"] == 0:\n",
        "            # Evaluate the current server parameters against the validation sets\n",
        "            client_losses_accuracies = [client.compute_accuracy(new_parameters) for client in clients]\n",
        "            client_losses, client_accuracies = zip(*client_losses_accuracies)\n",
        "\n",
        "            # Average accuracy across the clients\n",
        "            avg_client_accuracy = mean(client_acc for client_acc in client_accuracies)\n",
        "            accuracies[alpha].append(avg_client_accuracy * 100)\n",
        "            print(f\"Average accuracy after {step} rounds is {avg_client_accuracy*100}\")\n",
        "\n",
        "            # Periodically save the computations done so far\n",
        "            if step % config[\"SAVE_FREQUENCY\"] == 0:\n",
        "                printJSON(alpha, accuracies[alpha], net, step)\n",
        "\n",
        "            # Save the model with the best accuracy\n",
        "            if avg_client_accuracy >= best_accuracy:\n",
        "                best_accuracy = avg_client_accuracy\n",
        "                best_model = deepcopy(new_parameters)\n",
        "\n",
        "    # Print the final average accuracy and save the final model  \n",
        "    avg_accuracy = mean(float(client.compute_accuracy(best_model)[1]) for client in clients)\n",
        "  \n",
        "    config[\"AVERAGE_ACCURACY\"][alpha_i] = avg_accuracy\n",
        "    print(f\"Average accuracy with alpha = {alpha} after {step+1} rounds is {avg_accuracy*100}\")\n",
        "    printJSON(alpha, accuracies[alpha], net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFAUYXEglnu9"
      },
      "source": [
        "## Accuracy plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lheU1gStn_ZT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "ax = plt.figure().gca()\n",
        "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "\n",
        "x = np.arange(0, config[\"MAX_TIME\"], config[\"LOG_FREQUENCY\"])\n",
        "for alpha, values in accuracies.items():\n",
        "  plt.plot(x, values, label=f\"{alpha}\")\n",
        "\n",
        "plt.xlabel(\"Number of rounds\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy vs number of rounds\")\n",
        "plt.legend(title=\"Dirichlet alpha\")\n",
        "plt.savefig(\"FedDyn_non_iid.png\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_CydeCTlnu-"
      },
      "source": [
        "## Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-08T21:37:08.441628Z",
          "iopub.status.idle": "2022-02-08T21:37:08.442951Z",
          "shell.execute_reply": "2022-02-08T21:37:08.442703Z",
          "shell.execute_reply.started": "2022-02-08T21:37:08.442676Z"
        },
        "id": "KWMUrxVETSnt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "zip_name = \"artifacts\"\n",
        "if \"/\" in artifacts_target_directory:\n",
        "    zip_name = artifacts_target_directory.split(\"/\")[-1]\n",
        "\n",
        "shutil.make_archive(zip_name + \"-final\", \"zip\", f\"{artifacts_target_directory}/final\")\n",
        "shutil.make_archive(zip_name + \"-partials\", \"zip\", f\"{artifacts_target_directory}/partials\")\n",
        "\n",
        "# Display a link to the artifacts zip (useful on Kaggle and Colab)\n",
        "from IPython.display import FileLink\n",
        "FileLink(f\"{zip_name}-final.zip\")\n",
        "FileLink(f\"{zip_name}-partials.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5jL7Bdtydr5"
      },
      "outputs": [],
      "source": [
        "shutil.copy(f\"{zip_name}-final.zip\", artifacts_target_directory)\n",
        "shutil.copy(f\"{zip_name}-partials.zip\", artifacts_target_directory)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "aml-fed-dyn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
