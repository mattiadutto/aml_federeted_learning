{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aml.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYcjFeQ2udavuJdo4oudJi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattiadutto/aml_federeted_learning/blob/main/aml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ar1b6p68b3G",
        "outputId": "23db045c-5f9c-4f8c-982d-47fb11ab2cab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# From: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square, you can specify with a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "net = net.to(\"cuda\")\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "E = 2\n",
        "\n",
        "class Client():\n",
        "  def __init__(self, train_set, n):\n",
        "    self.train_set = train_set\n",
        "    self.n = n\n",
        "    self.batch_size = 32\n",
        "    self.train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "    self.net = Net()\n",
        "    self.net = self.net.to(\"cuda\")\n",
        "    # create your optimizer\n",
        "    self.optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "  def clientUpdate(self, parameters):\n",
        "    net.load_state_dict(parameters)\n",
        "    theta = parameters\n",
        "    for e in range(E):\n",
        "      for images, labels in self.train_loader:\n",
        "        images = images.to(\"cuda\")\n",
        "        labels = labels.to(\"cuda\")\n",
        "        # in your training loop:\n",
        "        self.optimizer.zero_grad()   # zero the gradient buffers\n",
        "        output = net(images)\n",
        "        loss = self.criterion(output, labels)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()    # Does the update\n",
        "    \n",
        "    return_dict = {}\n",
        "    for (k1, v1), (k2, v2) in zip(parameters.items(), net.state_dict().items()):\n",
        "      return_dict[k1] = v1 - lr * v2\n",
        "    return return_dict"
      ],
      "metadata": {
        "id": "eCwGFjE5Feil"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# K = 1, NUMBE_OR_CLIENTS = 2, MAX_TIME = 3 -> 58 sec\n",
        "\n",
        "K = 1 # to set\n",
        "NUMBER_OF_CLIENTS = 2 # to set\n",
        "MAX_TIME = 3 #to set\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform = transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "trainset_len = ( len(trainset) // NUMBER_OF_CLIENTS ) * NUMBER_OF_CLIENTS\n",
        "print(trainset_len)\n",
        "trainset = torch.utils.data.Subset(trainset, list(range(trainset_len)))\n",
        "\n",
        "lengths = len(trainset) // NUMBER_OF_CLIENTS * np.ones(NUMBER_OF_CLIENTS, dtype=np.int)\n",
        "print(lengths)\n",
        "trainsets = torch.utils.data.random_split(dataset=trainset, lengths=lengths)\n",
        "\n",
        "lr = 0.01 \n",
        "\n",
        "clientsSizes = torch.zeros(NUMBER_OF_CLIENTS)\n",
        "clients = list()\n",
        "\n",
        "def selectClients(k):\n",
        "  return random.sample(clients, k)\n",
        "\n",
        "def aggregateClient(deltaThetas):\n",
        "  parameters = None\n",
        "  for i,d in enumerate(deltaThetas):\n",
        "    if i == 0:\n",
        "      parameters = d\n",
        "    else:\n",
        "      for (k, v) in d.items():\n",
        "        parameters[k] += len(trainsets[i])/len(trainset) * v\n",
        "   \n",
        "  return parameters\n",
        "\n",
        "for c in range(NUMBER_OF_CLIENTS):\n",
        "  clients.append(Client(trainsets[c], len(trainset)))\n",
        "\n",
        "for t in range(MAX_TIME):\n",
        "  clients = selectClients(K)\n",
        "  deltaThetas = list()\n",
        "  for i, c in enumerate(clients):\n",
        "    deltaThetas.append(c.clientUpdate(net.state_dict()))\n",
        "  g = aggregateClient(deltaThetas)\n",
        "  \n",
        "  parameters = {}\n",
        "  for (k1, v1), (k2, v2) in zip(net.state_dict().items(), g.items()):\n",
        "    parameters[k1] = v1 - lr * v2\n",
        "  net.load_state_dict(parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTmw78eJ-EW5",
        "outputId": "3738ee8f-59b0-4f5b-d01a-8ff36ed4f4bf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "50000\n",
            "[25000 25000]\n"
          ]
        }
      ]
    }
  ]
}