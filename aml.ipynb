{"cells":[{"cell_type":"markdown","metadata":{"id":"KRyFWIxiPEVX"},"source":["# Baseline implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T09:13:28.090304Z","iopub.status.busy":"2022-02-04T09:13:28.089654Z","iopub.status.idle":"2022-02-04T09:13:28.095257Z","shell.execute_reply":"2022-02-04T09:13:28.094553Z","shell.execute_reply.started":"2022-02-04T09:13:28.090267Z"},"trusted":true},"outputs":[],"source":["# download the Cifar10 non-iid splits, if not present\n","\n","from os import path\n","import urllib.request\n","import zipfile\n","\n","if not path.exists(\"cifar10\"):\n","    save_path = \"cifar10.zip\"\n","    urllib.request.urlretrieve(\"http://storage.googleapis.com/gresearch/federated-vision-datasets/cifar10_v1.1.zip\", save_path)\n","    \n","    with zipfile.ZipFile(save_path, 'r') as zip_ref:\n","        zip_ref.extractall(\"cifar10\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T09:13:28.097846Z","iopub.status.busy":"2022-02-04T09:13:28.097135Z","iopub.status.idle":"2022-02-04T09:13:28.108201Z","shell.execute_reply":"2022-02-04T09:13:28.107515Z","shell.execute_reply.started":"2022-02-04T09:13:28.097802Z"},"trusted":true},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T09:13:28.109471Z","iopub.status.busy":"2022-02-04T09:13:28.109236Z","iopub.status.idle":"2022-02-04T09:13:28.117003Z","shell.execute_reply":"2022-02-04T09:13:28.116229Z","shell.execute_reply.started":"2022-02-04T09:13:28.109441Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T09:13:28.119467Z","iopub.status.busy":"2022-02-04T09:13:28.118947Z","iopub.status.idle":"2022-02-04T09:13:28.126427Z","shell.execute_reply":"2022-02-04T09:13:28.125781Z","shell.execute_reply.started":"2022-02-04T09:13:28.119427Z"},"trusted":true},"outputs":[],"source":["config = {\n","    \"E\": 1, # number of local epochs\n","    \"K\": 5, # number of clients selected each round # [5, 10, 20]\n","    \"NUMBER_OF_CLIENTS\": 100, # total number of clients\n","    \"MAX_TIME\": 2500,\n","    \"BATCH_SIZE\": 50,\n","    \"VALIDATION_BATCH_SIZE\": 500,\n","    \"LR\": 0.01,\n","    \"DATA_DISTRIBUTION\": \"non-iid\", # \"iid\" | \"non-iid\"\n","    \"DIRICHELET_ALPHA\": [0.00, 0.05, 0.10, 0.20, 0.50, 1.00, 10.00, 100.0],\n","    \"AVERAGE_ACCURACY\": np.zeros(8),\n","    \"FED_AVG_M\": False,\n","    \"FED_AVG_M_BETA\": 0.9,\n","    \"FED_AVG_M_GAMMA\": 1,\n","    \"LR_DECAY\": 0.99,\n","    \"LOG_FREQUENCY\": 25,\n","    \"AUGMENTATION_PROB\": 0.0,\n","    \"SAVE_FREQUENCY\": 100\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T09:13:28.128027Z","iopub.status.busy":"2022-02-04T09:13:28.127768Z","iopub.status.idle":"2022-02-04T09:13:28.138744Z","shell.execute_reply":"2022-02-04T09:13:28.138050Z","shell.execute_reply.started":"2022-02-04T09:13:28.127991Z"},"id":"5Ar1b6p68b3G","outputId":"e695f455-87f7-44c6-fe6f-fb6bdf6b300d","trusted":true},"outputs":[],"source":["import torch.optim as optim\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","# From: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n","class Net(nn.Module):\n","\n","    def __init__(self, *, input_size=32):\n","        super(Net, self).__init__()\n","        # 1 input image channel, 6 output channels, 5x5 square convolution\n","        self.conv1 = nn.Conv2d(3, 64, 5)\n","        self.conv2 = nn.Conv2d(64, 64, 5)\n","        \n","        # output of the conv layer is (w', h') = (w - 5 + 1, h - 5 + 1)\n","        # max_pool2d halves the dimensions (w', h') = (w / 2, h / 2)\n","\n","        # dynamically compute the image size\n","        size = input_size // 4 - 3\n","        self.fc1 = nn.Linear(64 * (size * size), 384)\n","        self.fc2 = nn.Linear(384, 192)\n","        self.fc3 = nn.Linear(192, 10)\n","\n","    def forward(self, x):\n","        # Max pooling over a (2, 2) window\n","        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n","        # If the size is a square, you can specify with a single number\n","        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n","        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","        \n","#print(net)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T09:13:28.140584Z","iopub.status.busy":"2022-02-04T09:13:28.140327Z","iopub.status.idle":"2022-02-04T09:13:28.157041Z","shell.execute_reply":"2022-02-04T09:13:28.156348Z","shell.execute_reply.started":"2022-02-04T09:13:28.140550Z"},"id":"eCwGFjE5Feil","trusted":true},"outputs":[],"source":["import torch.optim as optim\n","\n","class Client():\n","  def __init__(self, i, train_set, validation_set, *, input_size=32):\n","    self.i = i\n","    self.train_loader = torch.utils.data.DataLoader(train_set, batch_size=config[\"BATCH_SIZE\"],\n","                                         shuffle=True, num_workers=0, pin_memory = True)\n","    self.validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=config[\"VALIDATION_BATCH_SIZE\"],\n","                                         shuffle=False, num_workers=0)\n","    self.net = Net()\n","    self.net = self.net.to(device)\n","    # create your optimizer\n","    self.optimizer = optim.SGD(self.net.parameters(), lr=config[\"LR\"], weight_decay=4e-4)\n","    self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n","    # wandb.watch(self.net, criterion=self.criterion, log_freq=100, log_graph=False)\n","    \n","  def clientUpdate(self, lr, parameters):\n","    self.net.load_state_dict(parameters)\n","    self.net.train()\n","\n","    for g in self.optimizer.param_groups:\n","      g['lr'] = lr\n","\n","    for _ in range(config[\"E\"]):\n","      epoch_loss, n = 0, 0\n","      for images, labels in self.train_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        # in your training loop:\n","        self.optimizer.zero_grad()   # zero the gradient buffers\n","        outputs = self.net(images)\n","        loss = self.criterion(outputs, labels)\n","        epoch_loss += loss\n","        n += labels.size(0)\n","        \n","        loss = loss / labels.size(0)\n","        loss.backward()\n","        # wandb.log({f\"client-loss-{self.i}\": loss.item()})\n","        self.optimizer.step()    # Does the update\n","      epoch_loss = epoch_loss / n\n","\n","    return_dict = {}\n","    for (k1, v1), (k2, v2) in zip(parameters.items(), self.net.state_dict().items()):\n","      return_dict[k1] = v1 - v2\n","    return epoch_loss, return_dict\n","\n","  def compute_accuracy(self, parameters):\n","    self.net.load_state_dict(parameters)\n","    self.net.eval()\n","\n","    running_corrects = 0\n","    loss, n = 0, 0\n","    for data, labels in self.validation_loader:\n","        data = data.to(device)\n","        labels = labels.to(device)\n","\n","        with torch.no_grad():\n","          outputs = self.net(data)\n","        loss += self.criterion(outputs, labels).item()\n","\n","        _, preds = torch.max(outputs.data, 1)\n","\n","        running_corrects += torch.sum(preds == labels.data).data.item()\n","        n += len(preds)\n","                \n","    return loss/n, running_corrects / n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T09:13:28.223632Z","iopub.status.busy":"2022-02-04T09:13:28.222955Z","iopub.status.idle":"2022-02-04T09:13:28.230212Z","shell.execute_reply":"2022-02-04T09:13:28.229439Z","shell.execute_reply.started":"2022-02-04T09:13:28.223561Z"},"id":"QcsNecZCVPP-","trusted":true},"outputs":[],"source":["from collections import defaultdict\n","\n","def parse_csv(filename):\n","  splits = defaultdict(lambda: [])\n","  labels_mapping = dict()\n","\n","  with open(filename) as f:\n","    for line in f:\n","      if not line[0].isdigit():\n","        continue\n","\n","      user_id, image_id, label = (int(token) for token in line.split(\",\"))\n","      splits[user_id].append(image_id)\n","      labels_mapping[image_id] = label\n","\n","  return splits, labels_mapping"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T09:13:28.232422Z","iopub.status.busy":"2022-02-04T09:13:28.232058Z","iopub.status.idle":"2022-02-04T09:13:28.241840Z","shell.execute_reply":"2022-02-04T09:13:28.241091Z","shell.execute_reply.started":"2022-02-04T09:13:28.232345Z"},"trusted":true},"outputs":[],"source":["import time\n","import json\n","import numpy\n","from copy import deepcopy\n","\n","def listToString(l): \n","    return \" \".join(str(l))\n","\n","def printJSON(alpha, acc, net, step = None):\n","    artifacts_dir = \"artifacts\"\n","\n","    artifact_filename = f\"ALPHA_{alpha}_E_{config['E']}_K_{config['K']}\"\n","    if step is not None:\n","      artifact_filename += f\"_STEPS_{step}\"\n","      \n","    # parameters of the trained model\n","    server_model = net.state_dict()\n","    # save the model on the local file system\n","    torch.save(server_model, f\"{artifacts_dir}/{artifact_filename}.pth\")\n","    config_copy = deepcopy(config)\n","    config_copy[\"DIRICHELET_ALPHA\"] = listToString(config_copy[\"DIRICHELET_ALPHA\"])\n","    config_copy[\"AVERAGE_ACCURACY\"] = numpy.array2string(config_copy[\"AVERAGE_ACCURACY\"])\n","    data = {\n","        \"config\": config_copy,\n","        \"alpha\": listToString(alpha),\n","        \"accuracy\": acc\n","    }\n","\n","    with open(f\"{artifacts_dir}/{artifact_filename}.json\", \"w\") as f:\n","        f.write(json.dumps(data, indent=4))\n","\n","    # If you want to cat the file, my suggestion is to avoid this is a pretty heavy operation at least on my pc\n","    #artifact_filename += \".json\"\n","    #!cat artifact_filename\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T09:13:28.243566Z","iopub.status.busy":"2022-02-04T09:13:28.243196Z","iopub.status.idle":"2022-02-04T09:13:28.253135Z","shell.execute_reply":"2022-02-04T09:13:28.252399Z","shell.execute_reply.started":"2022-02-04T09:13:28.243471Z"},"trusted":true},"outputs":[],"source":["def selectClients(k):\n","  return random.sample(clients, k=k)\n","\n","def aggregateClient(deltaThetas):\n","  parameters = None\n","  for i,d in enumerate(deltaThetas):\n","    #ratio = len(trainsets[i])/len(trainset)\n","    ratio = len(trainsets[i])/(len(trainsets[i])*config['K'])\n","    \n","    if i == 0:\n","      parameters = {k:ratio*v for k, v in d.items()}\n","    else:\n","      for (k, v) in d.items():\n","        parameters[k] += ratio * v\n","   \n","  return parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T09:13:28.254754Z","iopub.status.busy":"2022-02-04T09:13:28.254199Z","iopub.status.idle":"2022-02-04T09:13:29.767940Z","shell.execute_reply":"2022-02-04T09:13:29.767205Z","shell.execute_reply.started":"2022-02-04T09:13:28.254713Z"},"id":"gTmw78eJ-EW5","outputId":"9bad269e-aa08-45ed-f934-27c805e12b15","trusted":true},"outputs":[],"source":["import torchvision\n","import torchvision.transforms as transforms\n","import numpy as np\n","import random\n","from statistics import mean\n","\n","from tqdm.notebook import tqdm\n","\n","import os\n","\n","random.seed(42)\n","\n","random_transform = transforms.Compose(\n","    [\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.RandomHorizontalFlip(1),\n","        transforms.ColorJitter(0.9, 0.9)\n","    ]\n",")\n","\n","\n","trainset = torchvision.datasets.CIFAR10(\n","    root='./data', \n","    train=True,\n","    download=True, \n","    transform=transforms.Compose(\n","        [\n","            transforms.ToTensor(),\n","            transforms.RandomApply([random_transform], config[\"AUGMENTATION_PROB\"]),\n","            transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262]), # from the net, there are the values of cifer10\n","        ]\n","    ),\n",")\n","\n","testset = torchvision.datasets.CIFAR10(\n","    root='./data', \n","    train=False,\n","    download=True, \n","    transform=transforms.Compose(\n","        [\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262]),\n","        ]\n","    ),\n","  )\n","\n","if not path.exists(\"artifacts\"):\n","  os.mkdir(\"artifacts\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T09:13:29.770321Z","iopub.status.busy":"2022-02-04T09:13:29.770046Z","iopub.status.idle":"2022-02-04T09:13:29.868798Z","shell.execute_reply":"2022-02-04T09:13:29.868149Z","shell.execute_reply.started":"2022-02-04T09:13:29.770284Z"},"trusted":true},"outputs":[],"source":["# verify the labels specified in the .csv files are coherent with the actual CIFAR-10 labels\n","# see https://github.com/google-research/google-research/issues/924\n","\n","_, labels_mapping = parse_csv(f\"cifar10/federated_train_alpha_{0.0:.2f}.csv\")\n","assert(all(label == labels_mapping[idx] for idx, label in enumerate(trainset.targets)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-04T09:13:29.870161Z","iopub.status.busy":"2022-02-04T09:13:29.869906Z","iopub.status.idle":"2022-02-04T09:14:40.956911Z","shell.execute_reply":"2022-02-04T09:14:40.955647Z","shell.execute_reply.started":"2022-02-04T09:13:29.870124Z"},"trusted":true},"outputs":[],"source":["for alpha_i, alpha in enumerate(config[\"DIRICHELET_ALPHA\"]):\n","  net = Net()\n","  net = net.to(device)\n","\n","  optimizer = optim.SGD(net.parameters(), lr=config[\"LR\"], momentum=0.9, weight_decay=1e-3)\n","  scheduler = ReduceLROnPlateau(optimizer, \"min\", factor=0.5, min_lr=1e-6, verbose=True)\n","\n","  if config[\"DATA_DISTRIBUTION\"] == \"iid\":\n","    # split the training set\n","    trainset_len = ( len(trainset) // config[\"NUMBER_OF_CLIENTS\"] ) * config[\"NUMBER_OF_CLIENTS\"]\n","    trainset = torch.utils.data.Subset(trainset, list(range(trainset_len)))\n","\n","    lengths = len(trainset) // config[\"NUMBER_OF_CLIENTS\"] * np.ones(config[\"NUMBER_OF_CLIENTS\"], dtype=int)\n","    trainsets = torch.utils.data.random_split(dataset=trainset, lengths=lengths)\n","  else:\n","    dirichelet_splits, _ = parse_csv(f\"cifar10/federated_train_alpha_{alpha:.2f}.csv\")\n","    trainsets = [torch.utils.data.Subset(trainset, indices) for indices in dirichelet_splits.values()]\n","\n","\n","  # split the validation set\n","  testset_len = ( len(testset) // config[\"NUMBER_OF_CLIENTS\"] ) * config[\"NUMBER_OF_CLIENTS\"]\n","  testset = torch.utils.data.Subset(testset, list(range(testset_len)))\n","\n","  lengths = len(testset) // config[\"NUMBER_OF_CLIENTS\"] * np.ones(config[\"NUMBER_OF_CLIENTS\"], dtype=int)\n","  testsets = torch.utils.data.random_split(dataset=testset, lengths=lengths)\n","\n","\n","  clientsSizes = torch.zeros(config[\"NUMBER_OF_CLIENTS\"])\n","  clients = list()\n","\n","\n","\n","  for c in range(config[\"NUMBER_OF_CLIENTS\"]):\n","    clients.append(Client(c, trainsets[c], testsets[c]))\n","\n","  if config[\"FED_AVG_M\"]:\n","    old_parameters = {}\n","\n","  # collect the test accuracies over the epochs\n","  test_accuracies = []\n","\n","  accuracies = list()\n","\n","  # best model\n","  best_model = {}\n","  best_accuracy = 0.0\n","\n","  for step in tqdm(range(config[\"MAX_TIME\"])):\n","    selected_clients = selectClients(config[\"K\"])\n","    #print(f\"Client(s) {[client.i for client in selected_clients]} selected\")\n","\n","    deltaThetas = list()\n","    losses = list()\n","    for i, c in enumerate(selected_clients):\n","      loss, parameters = c.clientUpdate(optimizer.param_groups[0]['lr'], net.state_dict())\n","      deltaThetas.append(parameters)\n","      losses.append(loss)\n","      \n","    g = aggregateClient(deltaThetas)\n","    \n","    parameters = {}\n","    for (k1, v1), (k2, v2) in zip(net.state_dict().items(), g.items()):\n","      \n","      if config[\"FED_AVG_M\"]:\n","        if k1 in old_parameters:\n","          parameters[k1] = v1 - config[\"FED_AVG_M_GAMMA\"] * (config[\"FED_AVG_M_BETA\"] * old_parameters[k1] + v2)  \n","          old_parameters[k1] = config[\"FED_AVG_M_BETA\"] * old_parameters[k1] + v2\n","        else:\n","          parameters[k1] = v1 - config[\"FED_AVG_M_GAMMA\"] * v2\n","          old_parameters[k1] = v2\n","      else:\n","        parameters[k1] = v1 - v2 # todo: add server learning rate gamma\n","\n","    # compute loss and accuracy on the test set of the clients\n","    # client.compute_accuracy(parameters) returns tuples (loss, accuracy)\n","    # client_losses_accuracies = [client.compute_accuracy(parameters) for client in clients]\n","    # client_losses, client_accuracies = zip(*client_losses_accuracies)\n","\n","    # compute the average client loss\n","    # and feed it to the scheduler\n","    # avg_client_loss = mean(client_loss for client_loss in client_losses)\n","    # scheduler.step(avg_client_loss)\n","\n","    # compute the average accuracy\n","    if step % config[\"LOG_FREQUENCY\"] == 0:\n","      client_losses_accuracies = [client.compute_accuracy(parameters) for client in clients]\n","      client_losses, client_accuracies = zip(*client_losses_accuracies)\n","      \n","      avg_client_accuracy = mean(client_acc for client_acc in client_accuracies)\n","      accuracies.append(avg_client_accuracy * 100)\n","      \n","      if avg_client_accuracy >= best_accuracy:\n","        best_accuracy = avg_client_accuracy\n","        best_model = net.state_dict()\n","          \n","      print(f\"Average accuracy after {step} rounds is {avg_client_accuracy*100}\")    \n","\n","    net.load_state_dict(parameters)\n","\n","    if step % config[\"SAVE_FREQUENCY\"] == 0:\n","      printJSON(alpha, accuracies, net, step)\n","  \n","  avg_accuracy = mean(float(client.compute_accuracy(best_model)[1]) for client in clients)\n","  #model_parameters = net.state_dict()\n","  #avg_accuracy = mean(float(client.compute_accuracy(model_parameters)[1]) for client in clients)\n"," \n","  #alpha = config[\"DIRICHELET_ALPHA\"][i]\n","  config[\"AVERAGE_ACCURACY\"][alpha_i] = avg_accuracy\n","  print(f\"Average accuracy with alpha = {alpha} after {step+1} rounds is {avg_accuracy*100}\")\n","  printJSON(alpha, accuracies, net)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import shutil\n","shutil.make_archive('artifacts', 'zip', 'artifacts')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"}},"nbformat":4,"nbformat_minor":4}
